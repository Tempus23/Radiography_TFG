{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import shutil\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator # type: ignore\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from src.config import *\n",
    "from src.data import *\n",
    "from src.models.efficientnet import EfficientNetB5Custom\n",
    "from src.utils import *\n",
    "from src.data import OriginalOAIDataset, dataset_augmentation\n",
    "from src.train import train, train_model\n",
    "from src.trainers.classification import Classification\n",
    "\n",
    "\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "torch.cuda.manual_seed(RANDOM_SEED)\n",
    "torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH_1 = MENDELEY_OAI_BRIGHT_200_PATH\n",
    "DATASET_PATH_2 = MENDELEY_EXPERT1_PATH # Resize to 224x224\n",
    "NEW_DATASET_PATH = 'dataset/OAI_mix_B200_E1'\n",
    "ORIGINAL_PATH = NEW_DATASET_PATH\n",
    "NEW_MIX_SPLIT_PATH = 'dataset/OAI_mix_B200_E1_split'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy dataset 1\n",
    "if not os.path.exists(NEW_DATASET_PATH):\n",
    "    shutil.copytree(DATASET_PATH_1, NEW_DATASET_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for class_name in os.listdir(DATASET_PATH_2):\n",
    "    class_path = os.path.join(DATASET_PATH_2, class_name)\n",
    "    print(class_path)\n",
    "    for img_name in os.listdir(class_path):\n",
    "        img_path = os.path.join(class_path, img_name)\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.resize(img, (224, 224))\n",
    "        new_img_path = os.path.join(NEW_DATASET_PATH, class_name, img_name)\n",
    "        cv2.imwrite(new_img_path, img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explorar_data(NEW_DATASET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ORIGINAL_PATH = NEW_DATASET_PATH\n",
    "NEW_MIX_SPLIT_PATH = 'dataset/OAI_mix_B200_E1_split'\n",
    "# Crear split para el dataset 70 10 20\n",
    "\n",
    "if not os.path.exists(NEW_MIX_SPLIT_PATH):\n",
    "    os.makedirs(NEW_MIX_SPLIT_PATH)\n",
    "\n",
    "NEW_TRAIN_PATH = os.path.join(NEW_MIX_SPLIT_PATH, 'train')\n",
    "NEW_VAL_PATH = os.path.join(NEW_MIX_SPLIT_PATH, 'val')\n",
    "NEW_TEST_PATH = os.path.join(NEW_MIX_SPLIT_PATH, 'test')\n",
    "\n",
    "for split in ['train', 'val', 'test']:\n",
    "    split_dir = os.path.join(NEW_MIX_SPLIT_PATH, split)\n",
    "    if not os.path.exists(split_dir):\n",
    "        os.makedirs(split_dir)\n",
    "    for c in os.listdir(ORIGINAL_PATH):\n",
    "        class_split_dir = os.path.join(split_dir, c)\n",
    "        if not os.path.exists(class_split_dir):\n",
    "            os.makedirs(class_split_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ORIGINAL_PATH = NEW_DATASET_PATH\n",
    "NEW_MIX_SPLIT_PATH = 'dataset/OAI_mix_B200_E1_split'\n",
    "classes = os.listdir(ORIGINAL_PATH)\n",
    "\n",
    "for c in classes:\n",
    "    class_dir = os.path.join(ORIGINAL_PATH, c)\n",
    "\n",
    "    images = os.listdir(class_dir)\n",
    "    n_images = len(images)\n",
    "    n_train = int(n_images * 0.7)\n",
    "    n_val = int(n_images * 0.1)\n",
    "    n_test = n_images - n_train - n_val\n",
    "\n",
    "    print(\"Generando split para la clase\", c)\n",
    "    print(\"Train:\", n_train)\n",
    "    print(\"Val:\", n_val)\n",
    "    print(\"Test:\", n_test)\n",
    "\n",
    "    train_images, val_test_images = train_test_split(images, test_size=n_val + n_test, random_state=RANDOM_SEED)\n",
    "    val_images, test_images = train_test_split(val_test_images, test_size=n_test, random_state=RANDOM_SEED)\n",
    "\n",
    "    for img in train_images:\n",
    "        shutil.move(os.path.join(class_dir, img), os.path.join(NEW_MIX_SPLIT_PATH, 'train', c, img))\n",
    "    for img in val_images:\n",
    "        shutil.move(os.path.join(class_dir, img), os.path.join(NEW_MIX_SPLIT_PATH, 'val', c, img))\n",
    "    for img in test_images:\n",
    "        shutil.move(os.path.join(class_dir, img), os.path.join(NEW_MIX_SPLIT_PATH, 'test', c, img))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╒═════════╤════════════╕\n",
      "│   Clase │   Cantidad │\n",
      "╞═════════╪════════════╡\n",
      "│       0 │        514 │\n",
      "├─────────┼────────────┤\n",
      "│       1 │        477 │\n",
      "├─────────┼────────────┤\n",
      "│       2 │        232 │\n",
      "├─────────┼────────────┤\n",
      "│       3 │        221 │\n",
      "├─────────┼────────────┤\n",
      "│       4 │        206 │\n",
      "╘═════════╧════════════╛\n",
      "╒═════════╤════════════╕\n",
      "│   Clase │   Cantidad │\n",
      "╞═════════╪════════════╡\n",
      "│       0 │       3115 │\n",
      "├─────────┼────────────┤\n",
      "│       1 │       1432 │\n",
      "├─────────┼────────────┤\n",
      "│       2 │       2056 │\n",
      "├─────────┼────────────┤\n",
      "│       3 │        998 │\n",
      "├─────────┼────────────┤\n",
      "│       4 │        225 │\n",
      "╘═════════╧════════════╛\n",
      "╒═════════╤══════╤══════╤══════╤═════╤═════╕\n",
      "│ Clase   │    0 │    1 │    2 │   3 │   4 │\n",
      "╞═════════╪══════╪══════╪══════╪═════╪═════╡\n",
      "│ train   │ 2540 │ 1336 │ 1601 │ 853 │ 301 │\n",
      "├─────────┼──────┼──────┼──────┼─────┼─────┤\n",
      "│ val     │  362 │  190 │  228 │ 121 │  43 │\n",
      "├─────────┼──────┼──────┼──────┼─────┼─────┤\n",
      "│ test    │  727 │  383 │  459 │ 245 │  87 │\n",
      "╘═════════╧══════╧══════╧══════╧═════╧═════╛\n",
      "1650 7826\n"
     ]
    }
   ],
   "source": [
    "n1 = sum(explorar_data(MENDELEY_EXPERT1_PATH))\n",
    "n2 = sum(explorar_data(MENDELEY_OAI_BRIGHT_200_PATH))\n",
    "n3 = explorar_split_data('dataset/OAI_mix_B200_E1_split')\n",
    "print(n1, n2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9476"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╒═════════╤══════╤══════╤══════╤══════╤══════╕\n",
      "│ Clase   │    0 │    1 │    2 │    3 │    4 │\n",
      "╞═════════╪══════╪══════╪══════╪══════╪══════╡\n",
      "│ train   │ 3000 │ 3000 │ 3000 │ 3000 │ 3000 │\n",
      "├─────────┼──────┼──────┼──────┼──────┼──────┤\n",
      "│ val     │  362 │  190 │  228 │  121 │   43 │\n",
      "├─────────┼──────┼──────┼──────┼──────┼──────┤\n",
      "│ test    │  727 │  383 │  459 │  245 │   87 │\n",
      "╘═════════╧══════╧══════╧══════╧══════╧══════╛\n",
      "17845\n"
     ]
    }
   ],
   "source": [
    "n1 = explorar_split_data('dataset/OAI_mix_B200_E1_augmented')\n",
    "num_total = sum(sum(n1[x]) for x in n1)\n",
    "print(num_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╒═════════╤══════╤══════╤══════╤══════╤══════╕\n",
      "│ Clase   │    0 │    1 │    2 │    3 │    4 │\n",
      "╞═════════╪══════╪══════╪══════╪══════╪══════╡\n",
      "│ train   │ 3000 │ 3000 │ 3000 │ 3000 │ 3000 │\n",
      "├─────────┼──────┼──────┼──────┼──────┼──────┤\n",
      "│ val     │  311 │  143 │  205 │   99 │   22 │\n",
      "├─────────┼──────┼──────┼──────┼──────┼──────┤\n",
      "│ test    │  624 │  287 │  412 │  201 │   46 │\n",
      "╘═════════╧══════╧══════╧══════╧══════╧══════╛\n",
      "17350\n"
     ]
    }
   ],
   "source": [
    "n1 = explorar_split_data('dataset/mendeleyOAI_dataset/brightness_200_split_aug_3k')\n",
    "num_total = sum(sum(n1[x]) for x in n1)\n",
    "print(num_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dataset/OAI_mix_B200_E1_split'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NEW_MIX_SPLIT_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying original images...\n",
      "Se han copiado 2540 imágenes de la clase 0\n",
      "Generando 460 imágenes aumentadas para la clase 0...\n",
      "Probabilidad de que se genere una imagen aumentada: 0.2311023622047244\n",
      "Se han generado 3000 imágenes aumentadas para la clase 0\n",
      "-----------------------------------\n",
      "\n",
      "Copying original images...\n",
      "Se han copiado 1336 imágenes de la clase 1\n",
      "Generando 1664 imágenes aumentadas para la clase 1...\n",
      "Probabilidad de que se genere una imagen aumentada: 1.2955089820359282\n",
      "Probabilidad de que se genere una imagen aumentada: 0.3486526946107784\n",
      "Probabilidad de que se genere una imagen aumentada: 0.0687125748502994\n",
      "Se han generado 3000 imágenes aumentadas para la clase 1\n",
      "-----------------------------------\n",
      "\n",
      "Copying original images...\n",
      "Se han copiado 1601 imágenes de la clase 2\n",
      "Generando 1399 imágenes aumentadas para la clase 2...\n",
      "Probabilidad de que se genere una imagen aumentada: 0.9238288569643973\n",
      "Probabilidad de que se genere una imagen aumentada: 0.05687070580886946\n",
      "Se han generado 3000 imágenes aumentadas para la clase 2\n",
      "-----------------------------------\n",
      "\n",
      "Copying original images...\n",
      "Se han copiado 853 imágenes de la clase 3\n",
      "Generando 2147 imágenes aumentadas para la clase 3...\n",
      "Probabilidad de que se genere una imagen aumentada: 2.5669988276670574\n",
      "Probabilidad de que se genere una imagen aumentada: 1.6068581477139507\n",
      "Probabilidad de que se genere una imagen aumentada: 0.7229191090269637\n",
      "Probabilidad de que se genere una imagen aumentada: 0.13440797186400938\n",
      "Se han generado 3000 imágenes aumentadas para la clase 3\n",
      "-----------------------------------\n",
      "\n",
      "Copying original images...\n",
      "Se han copiado 301 imágenes de la clase 4\n",
      "Generando 2699 imágenes aumentadas para la clase 4...\n",
      "Probabilidad de que se genere una imagen aumentada: 9.016777408637875\n",
      "Probabilidad de que se genere una imagen aumentada: 8.030066445182724\n",
      "Probabilidad de que se genere una imagen aumentada: 7.06328903654485\n",
      "Probabilidad de que se genere una imagen aumentada: 6.149667774086379\n",
      "Probabilidad de que se genere una imagen aumentada: 5.255980066445183\n",
      "Probabilidad de que se genere una imagen aumentada: 4.365614617940199\n",
      "Probabilidad de que se genere una imagen aumentada: 3.5117940199335544\n",
      "Probabilidad de que se genere una imagen aumentada: 2.6646179401993355\n",
      "Probabilidad de que se genere una imagen aumentada: 1.8672757475083057\n",
      "Probabilidad de que se genere una imagen aumentada: 1.11312292358804\n",
      "Probabilidad de que se genere una imagen aumentada: 0.3888704318936877\n",
      "Probabilidad de que se genere una imagen aumentada: 0.08654485049833888\n",
      "Se han generado 3000 imágenes aumentadas para la clase 4\n",
      "-----------------------------------\n",
      "\n",
      "Se han copiado 362 imágenes de la clase 0 en el conjunto val\n",
      "-----------------------------------\n",
      "\n",
      "Se han copiado 190 imágenes de la clase 1 en el conjunto val\n",
      "-----------------------------------\n",
      "\n",
      "Se han copiado 228 imágenes de la clase 2 en el conjunto val\n",
      "-----------------------------------\n",
      "\n",
      "Se han copiado 121 imágenes de la clase 3 en el conjunto val\n",
      "-----------------------------------\n",
      "\n",
      "Se han copiado 43 imágenes de la clase 4 en el conjunto val\n",
      "-----------------------------------\n",
      "\n",
      "Se han copiado 727 imágenes de la clase 0 en el conjunto test\n",
      "-----------------------------------\n",
      "\n",
      "Se han copiado 383 imágenes de la clase 1 en el conjunto test\n",
      "-----------------------------------\n",
      "\n",
      "Se han copiado 459 imágenes de la clase 2 en el conjunto test\n",
      "-----------------------------------\n",
      "\n",
      "Se han copiado 245 imágenes de la clase 3 en el conjunto test\n",
      "-----------------------------------\n",
      "\n",
      "Se han copiado 87 imágenes de la clase 4 en el conjunto test\n",
      "-----------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset_augmentation(NEW_MIX_SPLIT_PATH, 'dataset/OAI_mix_B200_E1_augmented', 3000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
