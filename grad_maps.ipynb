{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\34658\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\34658\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Import necessary packages and libraries\n",
    "import torchvision\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Load pre-trained model\n",
    "vgg_model = torchvision.models.vgg16(pretrained=True)\n",
    "\n",
    "# transformation for passing image into the network\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# selecting layers from the model to generate activations\n",
    "image_to_heatmaps = nn.Sequential(*list(vgg_model.features[:-4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_heatmap(model,img):\n",
    "  # compute logits from the model\n",
    "  logits = model(img)\n",
    "  # model's prediction \n",
    "  pred = logits.max(-1)[-1]\n",
    "  # activations from the model\n",
    "  activations = image_to_heatmaps(img)\n",
    "  # compute gradients with respect to the model's most confident prediction\n",
    "  logits[0, pred].backward(retain_graph=True)\n",
    "  # average gradients of the featuremap \n",
    "  pool_grads = model.efficientnet.features[-1].weight.grad.data.mean((0,2,3))\n",
    "  # multiply each activation map with corresponding gradient average\n",
    "  for i in range(activations.shape[1]):\n",
    "    activations[:,i,:,:] *= pool_grads[i]\n",
    "  # calculate mean of weighted activations\n",
    "  heatmap = torch.mean(activations, dim=1)[0].cpu().detach()\n",
    "  return heatmap, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsampleHeatmap(map, image):\n",
    "  # permute image\n",
    "  image = image.squeeze(0).permute(1, 2, 0).cpu().numpy()\n",
    "  # maximum and minimum value from heatmap\n",
    "  m, M = map.min(), map.max()\n",
    "  # normalize the heatmap\n",
    "  map = 255 * ((map-m)/ (m-M))\n",
    "  map = np.uint8(map)\n",
    "  # resize the heatmap to the same as the input\n",
    "  map = cv2.resize(map, (224, 224))\n",
    "  map = cv2.applyColorMap(255-map, cv2.COLORMAP_JET)\n",
    "  map = np.uint8(map)\n",
    "  # change this to balance between heatmap and image\n",
    "  map = np.uint8(map*0.7 + image*0.3)\n",
    "  return map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def display_images(upsampled_map, image):\n",
    "    image = image.squeeze(0).permute(1, 2, 0)\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "    axes[0].imshow(upsampled_map)\n",
    "    axes[0].set_title(\"Heatmap\")\n",
    "    axes[0].axis('off')\n",
    "    axes[1].imshow(image)\n",
    "    axes[1].set_title(\"Original Image\")\n",
    "    axes[1].axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.models.efficientnet import EfficientNetB5Custom\n",
    "model_state = torch.load('models/efficientnet/best_model_EfficientNetB5Custom_epoch_0.pt',map_location=torch.device('cpu'))\n",
    "\n",
    "model = EfficientNetB5Custom(num_classes=5, pretrained=False)\n",
    "\n",
    "model.load_state_dict(model_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function OrderedDict.keys>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Conv2dNormActivation' object has no attribute 'weight'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m normal_knee \u001b[38;5;241m=\u001b[39m transform(normal_knee)\n\u001b[0;32m      7\u001b[0m normal_knee \u001b[38;5;241m=\u001b[39m normal_knee\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m----> 8\u001b[0m heatmap,pred \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_heatmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnormal_knee\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m upsampled_map \u001b[38;5;241m=\u001b[39m upsampleHeatmap(heatmap, normal_knee)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrediction: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpred\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[15], line 11\u001b[0m, in \u001b[0;36mcompute_heatmap\u001b[1;34m(model, img)\u001b[0m\n\u001b[0;32m      9\u001b[0m logits[\u001b[38;5;241m0\u001b[39m, pred]\u001b[38;5;241m.\u001b[39mbackward(retain_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# average gradients of the featuremap \u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m pool_grads \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mefficientnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mmean((\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m3\u001b[39m))\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# multiply each activation map with corresponding gradient average\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(activations\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]):\n",
      "File \u001b[1;32mc:\\Users\\34658\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1614\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1612\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[0;32m   1613\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1614\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   1615\u001b[0m     \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Conv2dNormActivation' object has no attribute 'weight'"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "# Load an image\n",
    "normal_knee = r\"dataset\\mendeley_dataset\\MedicalExpert-I\\0\\NormalG0 (1).png\"\n",
    "normal_knee = Image.open(normal_knee)\n",
    "normal_knee = transform(normal_knee)\n",
    "\n",
    "normal_knee = normal_knee.unsqueeze(0)\n",
    "heatmap,pred = compute_heatmap(model,normal_knee)\n",
    "upsampled_map = upsampleHeatmap(heatmap, normal_knee)\n",
    "print(f\"Prediction: {pred}\")\n",
    "\n",
    "display_images(upsampled_map, normal_knee)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
