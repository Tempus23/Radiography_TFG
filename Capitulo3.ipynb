{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-04T09:52:44.302789Z",
     "iopub.status.busy": "2025-03-04T09:52:44.302566Z",
     "iopub.status.idle": "2025-03-04T09:52:44.306119Z",
     "shell.execute_reply": "2025-03-04T09:52:44.305395Z",
     "shell.execute_reply.started": "2025-03-04T09:52:44.302768Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "DATASET_PATH = '/kaggle/input/aug-oai-capitulo3'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análisis del Conjunto de Datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Distribución General\n",
    "\n",
    "El dataset contiene un **total de 9,786 imágenes** distribuidas en las siguientes clases:\n",
    "\n",
    "| **Clase** | **Cantidad de Imágenes** | **Porcentaje** |\n",
    "|----------|----------------------|--------------|\n",
    "| **Clase 0** | 3,857 | 39.41% |\n",
    "| **Clase 1** | 1,770 | 18.09% |\n",
    "| **Clase 2** | 2,578 | 26.34% |\n",
    "| **Clase 3** | 1,286 | 13.14% |\n",
    "| **Clase 4** | 295 | 3.01% |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## División del Conjunto de Datos\n",
    "\n",
    "Para el entrenamiento y evaluación del modelo, se realizó una división del conjunto de datos con las siguientes proporciones:\n",
    "- **Entrenamiento (Train)**: 75%\n",
    "- **Prueba (Test)**: 17%\n",
    "- **Validación (Val)**: 8%\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Conjunto de Entrenamiento (Train)\n",
    "**Total**: 7,337 imágenes (74.97% del dataset)\n",
    "\n",
    "| **Clase** | **Cantidad** | **Porcentaje dentro del conjunto** |\n",
    "|-----------|-------------|----------------------------------|\n",
    "| **Clase 0** | 2,892 | 39.42% |\n",
    "| **Clase 1** | 1,327 | 18.09% |\n",
    "| **Clase 2** | 1,933 | 26.35% |\n",
    "| **Clase 3** | 964 | 13.14% |\n",
    "| **Clase 4** | 221 | 3.01% |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Conjunto de Prueba (Test)\n",
    "**Total**: 1,669 imágenes (17.05% del dataset)\n",
    "\n",
    "| **Clase** | **Cantidad** | **Porcentaje dentro del conjunto** |\n",
    "|-----------|-------------|----------------------------------|\n",
    "| **Clase 0** | 657 | 39.36% |\n",
    "| **Clase 1** | 302 | 18.09% |\n",
    "| **Clase 2** | 439 | 26.30% |\n",
    "| **Clase 3** | 220 | 13.18% |\n",
    "| **Clase 4** | 51 | 3.06% |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Conjunto de Validación (Val)\n",
    "**Total**: 780 imágenes (7.97% del dataset)\n",
    "\n",
    "| **Clase** | **Cantidad** | **Porcentaje dentro del conjunto** |\n",
    "|-----------|-------------|----------------------------------|\n",
    "| **Clase 0** | 308 | 39.49% |\n",
    "| **Clase 1** | 141 | 18.08% |\n",
    "| **Clase 2** | 206 | 26.41% |\n",
    "| **Clase 3** | 102 | 13.08% |\n",
    "| **Clase 4** | 23 | 2.95% |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "imagenes = {}\n",
    "for subset in os.listdir(DATASET_PATH):\n",
    "    subset_path = os.path.join(DATASET_PATH, subset)\n",
    "    for kl_grade in os.listdir(subset_path):\n",
    "        kl_grade_path = os.path.join(subset_path, kl_grade)\n",
    "        for image in os.listdir(kl_grade_path):\n",
    "            image_path = os.path.join(kl_grade_path, image)\n",
    "            if kl_grade not in imagenes:\n",
    "                imagenes[kl_grade] = 0\n",
    "            imagenes[kl_grade] += 1\n",
    "\n",
    "print(f\"Total imagenes: {sum(imagenes.values())}\")\n",
    "for kl_grade, count in imagenes.items():\n",
    "    print(f\"Clase {kl_grade}: {count} \\t {count/sum(imagenes.values())*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comprobar imagenes repetidas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import hashlib\n",
    "hashes = []\n",
    "duplicates = 0\n",
    "\n",
    "for split in os.listdir(DATASET_PATH):\n",
    "    split_path = os.path.join(DATASET_PATH, split)\n",
    "    print(\"Analizando split \", split)\n",
    "    for kl_grade in os.listdir(split_path):\n",
    "        kl_grade_path = os.path.join(split_path, kl_grade)\n",
    "        for image in os.listdir(kl_grade_path):\n",
    "            image_path = os.path.join(kl_grade_path, image)\n",
    "            img = cv2.imread(image_path)\n",
    "            if img is None:\n",
    "                print(f\"Error al cargar {image_path}\")\n",
    "                continue\n",
    "            hash = hashlib.md5(img.tobytes()).hexdigest()\n",
    "            if hash in hashes:\n",
    "                print(f\"Imagen duplicada: {image_path} y {hashes[hash]}\")\n",
    "                duplicates += 1\n",
    "            else:\n",
    "                hashes.append(hash)\n",
    "print(duplicates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unificar todas las imagenes el en mismo directorio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNIFIED_DATASET_PATH = 'dataset/mendeleyOAI_dataset/ClsKLData/kneeKL224_unified'\n",
    "\n",
    "import shutil\n",
    "os.makedirs(UNIFIED_DATASET_PATH, exist_ok=True)\n",
    "for split in os.listdir(DATASET_PATH):\n",
    "    split_path = os.path.join(DATASET_PATH, split)\n",
    "    print(\"Copiando split \", split, end=\"\")\n",
    "    for kl_grade in os.listdir(split_path):\n",
    "        print(\".\", end=\"\")\n",
    "        os.makedirs(os.path.join(UNIFIED_DATASET_PATH, kl_grade), exist_ok=True)\n",
    "        kl_grade_path = os.path.join(split_path, kl_grade)\n",
    "        for image in os.listdir(kl_grade_path):\n",
    "            image_path = os.path.join(kl_grade_path, image)\n",
    "            shutil.copy(image_path, os.path.join(UNIFIED_DATASET_PATH, kl_grade, image))\n",
    "    print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "imagenes = {}\n",
    "for kl_grade in os.listdir(UNIFIED_DATASET_PATH):\n",
    "    kl_grade_path = os.path.join(UNIFIED_DATASET_PATH, kl_grade)\n",
    "    for image in os.listdir(kl_grade_path):\n",
    "        image_path = os.path.join(kl_grade_path, image)\n",
    "        if kl_grade not in imagenes:\n",
    "            imagenes[kl_grade] = 0\n",
    "        imagenes[kl_grade] += 1\n",
    "\n",
    "print(f\"Total imagenes: {sum(imagenes.values())}\")\n",
    "for kl_grade, count in imagenes.items():\n",
    "    print(f\"Clase {kl_grade}: {count} \\t {count/sum(imagenes.values())*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### División de Datos\n",
    "- **Entrenamiento (train):** 75%\n",
    "- **Validación (val):** 17%\n",
    "- **Prueba (test):** 8%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from src.config import RANDOM_SEED\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "train_ratio = 0.75\n",
    "val_ratio = 0.08\n",
    "test_ratio = 0.17\n",
    "\n",
    "assert train_ratio + val_ratio + test_ratio == 1\n",
    "# Crear directorios\n",
    "ORIGINAL_OAI_PATH = 'dataset/mendeleyOAI_dataset/ClsKLData/kneeKL224_unified'\n",
    "SPLIT_OAI_PATH = 'dataset/experimento1/split_oai'\n",
    "os.makedirs(SPLIT_OAI_PATH, exist_ok=True)\n",
    "for split in ['test','train','val']:\n",
    "    os.makedirs(os.path.join(SPLIT_OAI_PATH, split), exist_ok=True)\n",
    "    for kl_grade in os.listdir(ORIGINAL_OAI_PATH):\n",
    "        os.makedirs(os.path.join(SPLIT_OAI_PATH, split, kl_grade), exist_ok=True)\n",
    "\n",
    "# Train 0, 9001695L\n",
    "total_train_images = 0\n",
    "total_val_images = 0\n",
    "total_test_images = 0\n",
    "for class_name in os.listdir(ORIGINAL_OAI_PATH):\n",
    "    class_path = os.path.join(ORIGINAL_OAI_PATH, class_name)\n",
    "    images = os.listdir(class_path)\n",
    "    random.shuffle(images)\n",
    "    \n",
    "    n_train_images = int(len(images) * train_ratio)\n",
    "    n_val_images = int(len(images) * val_ratio)\n",
    "    n_test_images = len(images) - n_train_images - n_val_images\n",
    "    total_train_images += n_train_images\n",
    "    total_val_images += n_val_images\n",
    "    total_test_images += n_test_images\n",
    "    print(f\"Clase {class_name}: {len (images)} \\t  train {n_train_images} \\t val {n_val_images} \\t test {n_test_images}\")\n",
    "\n",
    "    for i, image in enumerate(images):\n",
    "        image_path = os.path.join(class_path, image)\n",
    "        if i < n_train_images:\n",
    "            shutil.copy(image_path, os.path.join(SPLIT_OAI_PATH, 'train', class_name, image))\n",
    "        elif i < n_train_images + n_val_images:\n",
    "            shutil.copy(image_path, os.path.join(SPLIT_OAI_PATH, 'val', class_name, image))\n",
    "        else:\n",
    "            shutil.copy(image_path, os.path.join(SPLIT_OAI_PATH, 'test', class_name, image))\n",
    "    \n",
    "\n",
    "\n",
    "print()\n",
    "print(f\"Train {total_train_images}, val {total_val_images}, test {total_test_images}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análisis split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "imagenes = {}\n",
    "for subset in os.listdir(SPLIT_OAI_PATH):\n",
    "    subset_path = os.path.join(SPLIT_OAI_PATH, subset)\n",
    "    imagenes[subset] = {}\n",
    "    for kl_grade in os.listdir(subset_path):\n",
    "        kl_grade_path = os.path.join(subset_path, kl_grade)\n",
    "        if kl_grade not in imagenes[subset]:\n",
    "            imagenes[subset][kl_grade] = 0\n",
    "        imagenes[subset][kl_grade] += len(os.listdir(kl_grade_path))\n",
    "\n",
    "# Print total images\n",
    "total_images = sum(sum(images.values()) for images in imagenes.values())\n",
    "print(f\"Total imagenes: {total_images}\")\n",
    "\n",
    "# Print statistics for each split\n",
    "for subset, classes in imagenes.items():\n",
    "    subset_total = sum(classes.values())\n",
    "    print(f\"\\n--- {subset.upper()} ---\")\n",
    "    print(f\"Total: {subset_total} ({subset_total/total_images*100:.2f}% of dataset)\")\n",
    "    \n",
    "    for kl_grade, count in sorted(classes.items()):\n",
    "        print(f\"  Clase {kl_grade}: {count} ({count/subset_total*100:.2f}% of {subset})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset con sus transformaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-04T10:03:26.613428Z",
     "iopub.status.busy": "2025-03-04T10:03:26.613098Z",
     "iopub.status.idle": "2025-03-04T10:03:26.899652Z",
     "shell.execute_reply": "2025-03-04T10:03:26.898994Z",
     "shell.execute_reply.started": "2025-03-04T10:03:26.613400Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "class HistogramEqualization:\n",
    "    \"\"\"Aplica ecualización de histograma para ajuste de contraste\"\"\"\n",
    "    def __call__(self, img):\n",
    "        # Convertir PIL Image a numpy array\n",
    "        img_np = np.array(img)\n",
    "        \n",
    "        # Aplicar ecualización de histograma por canal\n",
    "        if len(img_np.shape) == 3:  # Imagen RGB\n",
    "            img_eq = np.zeros_like(img_np)\n",
    "            for i in range(3):\n",
    "                img_eq[:,:,i] = cv2.equalizeHist(img_np[:,:,i])\n",
    "        else:  # Imagen en escala de grises\n",
    "            img_eq = cv2.equalizeHist(img_np)\n",
    "            \n",
    "        # Convertir de nuevo a PIL Image\n",
    "        return Image.fromarray(img_eq)\n",
    "\n",
    "class BilateralFilter:\n",
    "    \"\"\"Aplica filtrado bilateral para suavizado preservando bordes\"\"\"\n",
    "    def __init__(self, d=9, sigma_color=75, sigma_space=75):\n",
    "        self.d = d  # Diámetro de cada vecindario de píxeles\n",
    "        self.sigma_color = sigma_color  # Filtro sigma en el espacio de color\n",
    "        self.sigma_space = sigma_space  # Filtro sigma en el espacio de coordenadas\n",
    "    \n",
    "    def __call__(self, img):\n",
    "        # Convertir PIL Image a numpy array\n",
    "        img_np = np.array(img)\n",
    "        \n",
    "        # Aplicar filtro bilateral\n",
    "        img_filtered = cv2.bilateralFilter(\n",
    "            img_np, self.d, self.sigma_color, self.sigma_space)\n",
    "            \n",
    "        # Convertir de nuevo a PIL Image\n",
    "        return Image.fromarray(img_filtered)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-04T10:03:27.768039Z",
     "iopub.status.busy": "2025-03-04T10:03:27.767734Z",
     "iopub.status.idle": "2025-03-04T10:03:27.779149Z",
     "shell.execute_reply": "2025-03-04T10:03:27.778348Z",
     "shell.execute_reply.started": "2025-03-04T10:03:27.768013Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class DatasetExperiment1(Dataset):\n",
    "    def __init__(self, mode='train', batch_size=32, local = False, path = ''):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            mode (str): 'train', 'val' o 'test'.\n",
    "            transform: Transformaciones de torchvision a aplicar a las imágenes.\n",
    "        \"\"\"\n",
    "        assert mode in ['train', 'val', 'test'], \"Mode must be 'train', 'val', or 'test'\"\n",
    "        if local:\n",
    "            print(\"LOCAL MODE ENABLED\")\n",
    "\n",
    "        # Transformaciones del paper\n",
    "        # Histogram equalization for contrast adjustment\n",
    "        # and bilateral filtering for smoothness\n",
    "        self.transform =  transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            HistogramEqualization(),\n",
    "            BilateralFilter(d=9, sigma_color=75, sigma_space=75),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "        self.data_path = os.path.join(path, mode)\n",
    "        self.classes = sorted(os.listdir(self.data_path))  # Lista de clases\n",
    "        self.data = []\n",
    "        self.batch_size = batch_size\n",
    "        # Cargar imágenes con sus etiquetas\n",
    "        \n",
    "\n",
    "        for label, class_name in enumerate(self.classes):\n",
    "            class_path = os.path.join(self.data_path, class_name)\n",
    "            i = 0\n",
    "            for img_name in os.listdir(class_path):\n",
    "                if local and i >= 3:\n",
    "                    break\n",
    "                img_path = os.path.join(class_path, img_name)\n",
    "                self.data.append((img_path, label))\n",
    "                i += 1\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.data[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n",
    "    \n",
    "    def get_dataloader(self, shuffle=True):       \n",
    "        return DataLoader(self, batch_size=self.batch_size, shuffle=shuffle)\n",
    "    def show_image(self, idx, transformed=True, figsize=(10, 8)):\n",
    "        \"\"\"\n",
    "        Muestra una imagen del dataset con su etiqueta\n",
    "        \n",
    "        Args:\n",
    "            idx (int): Índice de la imagen a mostrar\n",
    "            transformed (bool): Si es True, muestra la imagen transformada. \n",
    "                               Si es False, muestra la imagen original.\n",
    "            figsize (tuple): Tamaño de la figura (ancho, alto)\n",
    "        \"\"\"\n",
    "        if idx >= len(self):\n",
    "            print(f\"Índice {idx} fuera de rango. El dataset tiene {len(self)} elementos.\")\n",
    "            return\n",
    "        \n",
    "        img_path, label = self.data[idx]\n",
    "        class_name = self.classes[label]\n",
    "        \n",
    "        plt.figure(figsize=figsize)\n",
    "        \n",
    "        # Mostrar imagen original\n",
    "        orig_img = Image.open(img_path).convert('RGB')\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(orig_img)\n",
    "        plt.title(f\"Original: Clase {class_name} (label {label})\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Mostrar imagen transformada si se solicita\n",
    "        if transformed:\n",
    "            trans_img = self.transform(orig_img)\n",
    "            # Convertir tensor a numpy para visualización\n",
    "            if isinstance(trans_img, torch.Tensor):\n",
    "                trans_img = trans_img.permute(1, 2, 0).numpy()  # Cambiar de CxHxW a HxWxC\n",
    "                # Normalizar valores para visualización\n",
    "                trans_img = np.clip(trans_img, 0, 1)\n",
    "            \n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.imshow(trans_img)\n",
    "            plt.title(\"Con transformaciones aplicadas\")\n",
    "            plt.axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Imprimir información adicional\n",
    "        print(f\"Ruta de la imagen: {img_path}\")\n",
    "        print(f\"Clase: {class_name} (label {label})\")\n",
    "        print(f\"Resolución original: {orig_img.size}\")\n",
    "        if transformed and isinstance(trans_img, np.ndarray):\n",
    "            print(f\"Resolución después de transformaciones: {trans_img.shape[:2]}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DatasetExperiment1('train', batch_size=32, local = True)\n",
    "dataset.show_image(10, transformed = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUGMENTED_DATASET_PATH = 'dataset/experimento1/augmented_oai'\n",
    "os.makedirs(AUGMENTED_DATASET_PATH, exist_ok=True)\n",
    "\n",
    "for split in ['train', 'val', 'test']:\n",
    "    os.makedirs(os.path.join(AUGMENTED_DATASET_PATH, split), exist_ok=True)\n",
    "    for kl_grade in os.listdir(ORIGINAL_OAI_PATH):\n",
    "        os.makedirs(os.path.join(AUGMENTED_DATASET_PATH, split, kl_grade), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Copy val and test images\n",
    "for split in ['val', 'test']:\n",
    "    for kl_grade in os.listdir(ORIGINAL_OAI_PATH):\n",
    "        kl_grade_path = os.path.join(SPLIT_OAI_PATH, split, kl_grade)\n",
    "        for image in os.listdir(kl_grade_path):\n",
    "            image_path = os.path.join(kl_grade_path, image)\n",
    "            shutil.copy(image_path, os.path.join(AUGMENTED_DATASET_PATH, split, kl_grade, image))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation for train\n",
    "\n",
    "for split in ['train']:\n",
    "    for kl_grade in os.listdir(ORIGINAL_OAI_PATH):\n",
    "        print(f\"Augmenting {kl_grade} images\")\n",
    "        kl_grade_path = os.path.join(SPLIT_OAI_PATH, split, kl_grade)\n",
    "        for image in os.listdir(kl_grade_path):\n",
    "            image_path = os.path.join(kl_grade_path, image)\n",
    "            shutil.copy(image_path, os.path.join(AUGMENTED_DATASET_PATH, split, kl_grade, image))\n",
    "            img = cv2.imread(image_path)\n",
    "            if img is None:\n",
    "                print(f\"Error al cargar {image_path}\")\n",
    "                continue\n",
    "            # Flip\n",
    "            img_flip = cv2.flip(img, 1)\n",
    "            cv2.imwrite(os.path.join(AUGMENTED_DATASET_PATH, split, kl_grade, f\"flip_{image}\"), img_flip)\n",
    "            # Rotate\n",
    "            img_rot = cv2.rotate(img, cv2.ROTATE_90_CLOCKWISE)\n",
    "            cv2.imwrite(os.path.join(AUGMENTED_DATASET_PATH, split, kl_grade, f\"rot_{image}\"), img_rot)\n",
    "            # Flip + Rotate\n",
    "            img_flip_rot = cv2.rotate(img_flip, cv2.ROTATE_90_CLOCKWISE)\n",
    "            cv2.imwrite(os.path.join(AUGMENTED_DATASET_PATH, split, kl_grade, f\"flip_rot_{image}\"), img_flip_rot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "imagenes = {}\n",
    "for subset in os.listdir(AUGMENTED_DATASET_PATH):\n",
    "    subset_path = os.path.join(AUGMENTED_DATASET_PATH, subset)\n",
    "    imagenes[subset] = {}\n",
    "    for kl_grade in os.listdir(subset_path):\n",
    "        kl_grade_path = os.path.join(subset_path, kl_grade)\n",
    "        if kl_grade not in imagenes[subset]:\n",
    "            imagenes[subset][kl_grade] = 0\n",
    "        imagenes[subset][kl_grade] += len(os.listdir(kl_grade_path))\n",
    "\n",
    "# Print total images\n",
    "total_images = sum(sum(images.values()) for images in imagenes.values())\n",
    "print(f\"Total imagenes: {total_images}\")\n",
    "\n",
    "# Print statistics for each split\n",
    "for subset, classes in imagenes.items():\n",
    "    subset_total = sum(classes.values())\n",
    "    print(f\"\\n--- {subset.upper()} ---\")\n",
    "    print(f\"Total: {subset_total} ({subset_total/total_images*100:.2f}% of dataset)\")\n",
    "    \n",
    "    for kl_grade, count in sorted(classes.items()):\n",
    "        print(f\"  Clase {kl_grade}: {count} ({count/subset_total*100:.2f}% of {subset})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train config 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hyperparameters of the proposed ensemble model were\n",
    "tuned by the trial-and-error method. \n",
    "L2 regularization, \n",
    "batch normalization, \n",
    "dropout rate\n",
    "\n",
    "The Adam optimizer\n",
    "Epochs = 150. \n",
    "The learning rate scheduler and ReduceLROnPlateau were used for handling\n",
    "the learning rate with an initial value of 0.001. The loss\n",
    "function used for the model was categorical cross-entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator # type: ignore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from src.config import *\n",
    "from src.data import *\n",
    "from src.models.efficientnet import EfficientNetB5Custom, EfficientNetB0, EfficientNetB5, EfficientNetB7, EfficientNetB4\n",
    "from src.models.resnet18 import ResNet18_v2\n",
    "from src.models.ensembling import Ensembling\n",
    "from src.utils import *\n",
    "from src.data import OriginalOAIDataset, DatasetExperiment1\n",
    "from src.train import train, train_model, test_model\n",
    "from src.trainers.classification import Classification, ClassificationAdamax\n",
    "from src.trainers.regresion import Regression\n",
    "import wandb\n",
    "# Set random seeds\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.cuda.manual_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, Tensor\n",
    "import torch\n",
    "from typing import Type\n",
    "from src.models.efficientnet import EfficientNetB0, EfficientNetB4\n",
    "\n",
    "# Clase Ensembling\n",
    "class Ensembling(nn.Module):\n",
    "    def __init__(self, num_classes: int, pretrained: bool = False) -> None:\n",
    "        super(Ensembling, self).__init__()\n",
    "        self.name = \"EnsemblingFaruq\"\n",
    "        self.efficientnetb0 = EfficientNetB0(num_classes=num_classes, pretrained=pretrained)\n",
    "        self.efficientnetb4 = EfficientNetB4(num_classes=num_classes, pretrained=pretrained)\n",
    "\n",
    "        self.b0_features = 1280\n",
    "        self.b4_features = 1790\n",
    "\n",
    "\n",
    "        #Clasificación 512 -> 256 -> 128 -> 65 -> num_classes con batchNormalization, l2regularization y dropout\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(150528, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Dropout(0.4),\n",
    "            \n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(128),\n",
    "            \n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(64),\n",
    "            \n",
    "            nn.Linear(64, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "\n",
    "        # Parte 1 - Features from efficientNet b0 and b4\n",
    "        features_b0 = self.efficientnetb0.features(x)\n",
    "        features_b4 = self.efficientnetb4.features(x)\n",
    "\n",
    "        # Flatten\n",
    "        features_b0 = torch.flatten(features_b0, 1)\n",
    "        features_b4 = torch.flatten(features_b4, 1)\n",
    "\n",
    "        #Concatenar\n",
    "        x = torch.cat((features_b0, features_b4), dim=1)\n",
    "\n",
    "        #Clasificación 512 -> 256 -> 128 -> 65 -> num_classes con batchNormalization, l2regularization y dropout\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOCAL MODE ENABLED\n",
      "LOCAL MODE ENABLED\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "PytorchStreamReader failed reading zip archive: failed finding central directory",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m val_dataset \u001b[38;5;241m=\u001b[39m DatasetExperiment1(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39mBATCH_SIZE, local\u001b[38;5;241m=\u001b[39mLOCAL, grey\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, path \u001b[38;5;241m=\u001b[39m DATASET_PATH)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m#model =ResNet18_v2(n_classes = 5)\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mEnsembling\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpretrained\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     18\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Classification(model, device, L1\u001b[38;5;241m=\u001b[39mL1, L2\u001b[38;5;241m=\u001b[39mL2, lr\u001b[38;5;241m=\u001b[39mLEARNING_RATE, factor\u001b[38;5;241m=\u001b[39mFACTOR, patience\u001b[38;5;241m=\u001b[39mPATIENCE, betas\u001b[38;5;241m=\u001b[39mBETAS)\n",
      "Cell \u001b[1;32mIn[10], line 12\u001b[0m, in \u001b[0;36mEnsembling.__init__\u001b[1;34m(self, num_classes, pretrained)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnsemblingFaruq\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mefficientnetb0 \u001b[38;5;241m=\u001b[39m EfficientNetB0(num_classes\u001b[38;5;241m=\u001b[39mnum_classes, pretrained\u001b[38;5;241m=\u001b[39mpretrained)\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mefficientnetb4 \u001b[38;5;241m=\u001b[39m \u001b[43mEfficientNetB4\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpretrained\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpretrained\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb0_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1280\u001b[39m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb4_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1790\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\34658\\OneDrive\\Escritorio\\Ing informática\\TFG\\Radiography_TFG\\src\\models\\efficientnet.py:65\u001b[0m, in \u001b[0;36mEfficientNetB4.__init__\u001b[1;34m(self, num_classes, pretrained)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28msuper\u001b[39m(EfficientNetB4, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pretrained:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mefficientnet \u001b[38;5;241m=\u001b[39m \u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mefficientnet_b4\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEfficientNet_B4_Weights\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDEFAULT\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mefficientnet \u001b[38;5;241m=\u001b[39m models\u001b[38;5;241m.\u001b[39mefficientnet_b4(weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\34658\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:142\u001b[0m, in \u001b[0;36mkwonly_to_pos_or_kw.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    135\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    136\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msequence_to_str(\u001b[38;5;28mtuple\u001b[39m(keyword_only_kwargs\u001b[38;5;241m.\u001b[39mkeys()),\u001b[38;5;250m \u001b[39mseparate_last\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mand \u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m as positional \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    137\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter(s) is deprecated since 0.13 and may be removed in the future. Please use keyword parameter(s) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    138\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    139\u001b[0m     )\n\u001b[0;32m    140\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39mupdate(keyword_only_kwargs)\n\u001b[1;32m--> 142\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\34658\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:228\u001b[0m, in \u001b[0;36mhandle_legacy_interface.<locals>.outer_wrapper.<locals>.inner_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    225\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m kwargs[pretrained_param]\n\u001b[0;32m    226\u001b[0m     kwargs[weights_param] \u001b[38;5;241m=\u001b[39m default_weights_arg\n\u001b[1;32m--> 228\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m builder(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\34658\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\efficientnet.py:899\u001b[0m, in \u001b[0;36mefficientnet_b4\u001b[1;34m(weights, progress, **kwargs)\u001b[0m\n\u001b[0;32m    896\u001b[0m weights \u001b[38;5;241m=\u001b[39m EfficientNet_B4_Weights\u001b[38;5;241m.\u001b[39mverify(weights)\n\u001b[0;32m    898\u001b[0m inverted_residual_setting, last_channel \u001b[38;5;241m=\u001b[39m _efficientnet_conf(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mefficientnet_b4\u001b[39m\u001b[38;5;124m\"\u001b[39m, width_mult\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.4\u001b[39m, depth_mult\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.8\u001b[39m)\n\u001b[1;32m--> 899\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _efficientnet(\n\u001b[0;32m    900\u001b[0m     inverted_residual_setting,\n\u001b[0;32m    901\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropout\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0.4\u001b[39m),\n\u001b[0;32m    902\u001b[0m     last_channel,\n\u001b[0;32m    903\u001b[0m     weights,\n\u001b[0;32m    904\u001b[0m     progress,\n\u001b[0;32m    905\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    906\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\34658\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\efficientnet.py:360\u001b[0m, in \u001b[0;36m_efficientnet\u001b[1;34m(inverted_residual_setting, dropout, last_channel, weights, progress, **kwargs)\u001b[0m\n\u001b[0;32m    357\u001b[0m model \u001b[38;5;241m=\u001b[39m EfficientNet(inverted_residual_setting, dropout, last_channel\u001b[38;5;241m=\u001b[39mlast_channel, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    359\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weights \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 360\u001b[0m     model\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mweights\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprogress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    362\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[1;32mc:\\Users\\34658\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_api.py:89\u001b[0m, in \u001b[0;36mWeightsEnum.get_state_dict\u001b[1;34m(self, progress)\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_state_dict\u001b[39m(\u001b[38;5;28mself\u001b[39m, progress: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Mapping[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[1;32m---> 89\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload_state_dict_from_url\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\34658\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\hub.py:750\u001b[0m, in \u001b[0;36mload_state_dict_from_url\u001b[1;34m(url, model_dir, map_location, progress, check_hash, file_name)\u001b[0m\n\u001b[0;32m    748\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_legacy_zip_format(cached_file):\n\u001b[0;32m    749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _legacy_zip_load(cached_file, model_dir, map_location)\n\u001b[1;32m--> 750\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcached_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmap_location\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\34658\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\serialization.py:797\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[0;32m    792\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[0;32m    793\u001b[0m     \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[0;32m    794\u001b[0m     \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[0;32m    795\u001b[0m     \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[0;32m    796\u001b[0m     orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n\u001b[1;32m--> 797\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_zipfile_reader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopened_file\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m    798\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m _is_torchscript_zip(opened_zipfile):\n\u001b[0;32m    799\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtorch.load\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m received a zip file that looks like a TorchScript archive\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    800\u001b[0m                           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m dispatching to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtorch.jit.load\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m (call \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtorch.jit.load\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m directly to\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    801\u001b[0m                           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m silence this warning)\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;167;01mUserWarning\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\34658\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\serialization.py:283\u001b[0m, in \u001b[0;36m_open_zipfile_reader.__init__\u001b[1;34m(self, name_or_buffer)\u001b[0m\n\u001b[0;32m    282\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name_or_buffer) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 283\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPyTorchFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: PytorchStreamReader failed reading zip archive: failed finding central directory"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 4\n",
    "LEARNING_RATE = 0.0001\n",
    "FACTOR = 0.1\n",
    "L1 = 0\n",
    "L2 = 0\n",
    "PATIENCE = 8\n",
    "BETAS=(0.9, 0.999)\n",
    "LOCAL = True\n",
    "# Regularización L1 y L2\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# wandb 254f72e7bdeec44797ba1b2a91ebbc63900b89f4\n",
    "DATASET_PATH = 'dataset/experimento1/augmented_oai'\n",
    "train_dataset = DatasetExperiment1('train', batch_size=BATCH_SIZE, local=LOCAL,grey=False, path = DATASET_PATH)\n",
    "val_dataset = DatasetExperiment1('val', batch_size=BATCH_SIZE, local=LOCAL, grey=False, path = DATASET_PATH)\n",
    "#model =ResNet18_v2(n_classes = 5)\n",
    "model = Ensembling(num_classes = 5, pretrained=True)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "trainer = Classification(model, device, L1=L1, L2=L2, lr=LEARNING_RATE, factor=FACTOR, patience=PATIENCE, betas=BETAS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch [1/20]:   0%|                                                                                                                                                      | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch [1/20]: 100%|███████████████████████████████████████████| 4/4 [00:42<00:00, 10.63s/it, AUC=0.472, acc=0.133, complete_loss=1.7615, sensivity=0.133, specificity=0.783, train_loss=1.7615]\n",
      "Validation Epoch [1/20]: 100%|████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:06<00:00,  1.54s/it, AUC=0.5, acc=0.2, specificity=0.8, val_loss=1.60977159]\n",
      "Training Epoch [2/20]: 100%|███████████████████████████████████████████| 4/4 [00:30<00:00,  7.66s/it, AUC=0.561, acc=0.333, complete_loss=1.5755, sensivity=0.333, specificity=0.833, train_loss=1.5755]\n",
      "Validation Epoch [2/20]: 100%|██████████████████████████████████████████████████████████████████████████████████| 4/4 [00:04<00:00,  1.21s/it, AUC=0.483, acc=0.2, specificity=0.8, val_loss=1.61027130]\n",
      "Training Epoch [3/20]: 100%|█████████████████████████████████████████████████| 4/4 [00:30<00:00,  7.55s/it, AUC=0.572, acc=0.2, complete_loss=1.5861, sensivity=0.2, specificity=0.8, train_loss=1.5861]\n",
      "Validation Epoch [3/20]: 100%|██████████████████████████████████████████████████████████████████████████████████| 4/4 [00:05<00:00,  1.28s/it, AUC=0.575, acc=0.2, specificity=0.8, val_loss=1.61077968]\n",
      "Training Epoch [4/20]:  75%|█████████████████████████████████           | 3/4 [00:29<00:09,  9.75s/it, AUC=0.425, acc=0.0833, complete_loss=1.8727, sensivity=0.1, specificity=0.769, train_loss=1.8727]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_model\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwdb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mLOCAL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping_patience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m12\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\34658\\OneDrive\\Escritorio\\Ing informática\\TFG\\Radiography_TFG\\src\\train.py:38\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, trainer, train_dataset, val_dataset, epochs, transform, device, save_model, name, wdb, local, project, early_stopping_patience, plot_loss)\u001b[0m\n\u001b[0;32m     36\u001b[0m val_loader \u001b[38;5;241m=\u001b[39m val_dataset\u001b[38;5;241m.\u001b[39mget_dataloader(shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     37\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 38\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwdb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[43m                \u001b[49m\u001b[43msave_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping_patience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mearly_stopping_patience\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Plot training & validation loss if requested\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m plot_loss \u001b[38;5;129;01mand\u001b[39;00m history:\n",
      "File \u001b[1;32mc:\\Users\\34658\\OneDrive\\Escritorio\\Ing informática\\TFG\\Radiography_TFG\\src\\train.py:81\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_loader, val_loader, trainer, epochs, device, wdb, local, save_model, early_stopping_patience)\u001b[0m\n\u001b[0;32m     79\u001b[0m batch \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device), batch[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     80\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 81\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     82\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     84\u001b[0m training_loss\u001b[38;5;241m.\u001b[39mappend(res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[1;32mc:\\Users\\34658\\OneDrive\\Escritorio\\Ing informática\\TFG\\Radiography_TFG\\src\\trainers\\classification.py:46\u001b[0m, in \u001b[0;36mClassification.training_step\u001b[1;34m(self, x, y)\u001b[0m\n\u001b[0;32m     44\u001b[0m L1_reg \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m0.\u001b[39m, requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mparameters():\n\u001b[1;32m---> 46\u001b[0m     L1_reg \u001b[38;5;241m=\u001b[39m L1_reg \u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mabs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# Regularización L2\u001b[39;00m\n\u001b[0;32m     49\u001b[0m L2_reg \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m0.\u001b[39m, requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_model(model, trainer, train_dataset, val_dataset, epochs=20, device=device, save_model = True, name=\"\", wdb=not LOCAL, local=False, early_stopping_patience=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([15, 3, 224, 224])\n",
      "torch.Size([15])\n",
      "torch.Size([15])\n",
      "tensor([0.2311], grad_fn=<SelectBackward0>) tensor(3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<Figure size 640x480 with 1 Axes>,\n",
       " <Axes: xlabel='Predicted class', ylabel='True class'>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchmetrics.classification import MulticlassConfusionMatrix\n",
    "for input, label in train_dataset.get_dataloader():\n",
    "    print(input.shape)\n",
    "    print(label.shape)\n",
    "    y = model(input)\n",
    "    print(y.squeeze().shape)\n",
    "\n",
    "    print(y[0], label[0])\n",
    "    break\n",
    "\n",
    "cm = MulticlassConfusionMatrix(num_classes=5).to(device)\n",
    "cm.update(y.squeeze(), label)\n",
    "cm.plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6786867,
     "sourceId": 10917159,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
