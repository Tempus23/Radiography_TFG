{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importar librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from src.config import *\n",
    "from src.data import *\n",
    "from src.utils import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mendeley 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explorar_split_data(path):\n",
    "    imagenes = {}\n",
    "    for subset in [\"auto_test\", \"train\", \"val\", \"test\"]:\n",
    "        subset_path = os.path.join(path, subset)\n",
    "        if not os.path.exists(subset_path):\n",
    "            continue\n",
    "        if os.path.exists(subset_path):\n",
    "            imagenes[subset] = []  # Inicializamos la lista para cada subset\n",
    "            for root, dirs, files in os.walk(subset_path):\n",
    "                # Omitimos el directorio raíz\n",
    "                if root == subset_path:\n",
    "                    continue\n",
    "                imagenes[subset].append(len(files))\n",
    "    print_split_table(imagenes)\n",
    "    return imagenes\n",
    "def print_split_table(data):\n",
    "    num_clases = len(data['train'])\n",
    "    tabla = []\n",
    "\n",
    "    for i in [\"auto_test\", \"train\", \"val\", \"test\"]:\n",
    "        fila = [f\"{i}\", data[i][0], data[i][1], data[i][2], data[i][3], data[i][4]]\n",
    "        tabla.append(fila)\n",
    "\n",
    "    print(tabulate(tabla, headers=[\"Clase\", \"0\", \"1\", \"2\", \"3\", \"4\"], tablefmt=\"fancy_grid\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╒═══════════╤══════╤══════╤══════╤═════╤═════╕\n",
      "│ Clase     │    0 │    1 │    2 │   3 │   4 │\n",
      "╞═══════════╪══════╪══════╪══════╪═════╪═════╡\n",
      "│ auto_test │  604 │  275 │  403 │ 200 │  44 │\n",
      "├───────────┼──────┼──────┼──────┼─────┼─────┤\n",
      "│ train     │ 2286 │ 1046 │ 1516 │ 757 │ 173 │\n",
      "├───────────┼──────┼──────┼──────┼─────┼─────┤\n",
      "│ val       │  328 │  153 │  212 │ 106 │  27 │\n",
      "├───────────┼──────┼──────┼──────┼─────┼─────┤\n",
      "│ test      │  639 │  296 │  447 │ 223 │  51 │\n",
      "╘═══════════╧══════╧══════╧══════╧═════╧═════╛\n",
      "Split auto_test: 1526 - 15.59%\n",
      "Split train: 5778 - 59.04%\n",
      "Split val: 826 - 8.44%\n",
      "Split test: 1656 - 16.92%\n",
      "9786\n",
      "[3857, 1770, 2578, 1286, 295]\n",
      "{'auto_test': [604, 275, 403, 200, 44], 'train': [2286, 1046, 1516, 757, 173], 'val': [328, 153, 212, 106, 27], 'test': [639, 296, 447, 223, 51]}\n"
     ]
    }
   ],
   "source": [
    "img = explorar_split_data('dataset/mendeleyOAI_dataset/ClsKLData/kneeKL224')\n",
    "TOTAL = sum([sum(img[split]) for split in img])\n",
    "num_por_clase = [0,0,0,0,0]\n",
    "for split in img:\n",
    "    print(f\"Split {split}: {sum(img[split])} - {sum(img[split])/TOTAL*100:.2f}%\") \n",
    "    num_por_clase = [num_por_clase[i] + img[split][i] for i in range(5)]\n",
    "\n",
    "\n",
    "print(TOTAL)\n",
    "print(num_por_clase)\n",
    "print(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analizando clase: 0\n",
      "  Total imágenes en 0: 604\n",
      "  Imágenes únicas: 604\n",
      "  Grupos de duplicados: 0\n",
      "  Total duplicados: 0\n",
      "\n",
      "Analizando clase: 1\n",
      "  Total imágenes en 1: 275\n",
      "  Imágenes únicas: 275\n",
      "  Grupos de duplicados: 0\n",
      "  Total duplicados: 0\n",
      "\n",
      "Analizando clase: 2\n",
      "  Total imágenes en 2: 403\n",
      "  Imágenes únicas: 403\n",
      "  Grupos de duplicados: 0\n",
      "  Total duplicados: 0\n",
      "\n",
      "Analizando clase: 3\n",
      "  Total imágenes en 3: 200\n",
      "  Imágenes únicas: 200\n",
      "  Grupos de duplicados: 0\n",
      "  Total duplicados: 0\n",
      "\n",
      "Analizando clase: 4\n",
      "  Total imágenes en 4: 44\n",
      "  Imágenes únicas: 44\n",
      "  Grupos de duplicados: 0\n",
      "  Total duplicados: 0\n",
      "\n",
      "Analizando clase: 0\n",
      "  Total imágenes en 0: 639\n",
      "  Imágenes únicas: 639\n",
      "  Grupos de duplicados: 0\n",
      "  Total duplicados: 0\n",
      "\n",
      "Analizando clase: 1\n",
      "  Total imágenes en 1: 296\n",
      "  Imágenes únicas: 296\n",
      "  Grupos de duplicados: 0\n",
      "  Total duplicados: 0\n",
      "\n",
      "Analizando clase: 2\n",
      "  Total imágenes en 2: 447\n",
      "  Imágenes únicas: 447\n",
      "  Grupos de duplicados: 0\n",
      "  Total duplicados: 0\n",
      "\n",
      "Analizando clase: 3\n",
      "  Total imágenes en 3: 223\n",
      "  Imágenes únicas: 223\n",
      "  Grupos de duplicados: 0\n",
      "  Total duplicados: 0\n",
      "\n",
      "Analizando clase: 4\n",
      "  Total imágenes en 4: 51\n",
      "  Imágenes únicas: 51\n",
      "  Grupos de duplicados: 0\n",
      "  Total duplicados: 0\n",
      "\n",
      "Analizando clase: 0\n",
      "  Total imágenes en 0: 2286\n",
      "  Imágenes únicas: 2286\n",
      "  Grupos de duplicados: 0\n",
      "  Total duplicados: 0\n",
      "\n",
      "Analizando clase: 1\n",
      "  Total imágenes en 1: 1046\n",
      "  Imágenes únicas: 1046\n",
      "  Grupos de duplicados: 0\n",
      "  Total duplicados: 0\n",
      "\n",
      "Analizando clase: 2\n",
      "  Total imágenes en 2: 1516\n",
      "  Imágenes únicas: 1516\n",
      "  Grupos de duplicados: 0\n",
      "  Total duplicados: 0\n",
      "\n",
      "Analizando clase: 3\n",
      "  Total imágenes en 3: 757\n",
      "  Imágenes únicas: 757\n",
      "  Grupos de duplicados: 0\n",
      "  Total duplicados: 0\n",
      "\n",
      "Analizando clase: 4\n",
      "  Total imágenes en 4: 173\n",
      "  Imágenes únicas: 173\n",
      "  Grupos de duplicados: 0\n",
      "  Total duplicados: 0\n",
      "\n",
      "Analizando clase: 0\n",
      "  Total imágenes en 0: 328\n",
      "  Imágenes únicas: 328\n",
      "  Grupos de duplicados: 0\n",
      "  Total duplicados: 0\n",
      "\n",
      "Analizando clase: 1\n",
      "  Total imágenes en 1: 153\n",
      "  Imágenes únicas: 153\n",
      "  Grupos de duplicados: 0\n",
      "  Total duplicados: 0\n",
      "\n",
      "Analizando clase: 2\n",
      "  Total imágenes en 2: 212\n",
      "  Imágenes únicas: 212\n",
      "  Grupos de duplicados: 0\n",
      "  Total duplicados: 0\n",
      "\n",
      "Analizando clase: 3\n",
      "  Total imágenes en 3: 106\n",
      "  Imágenes únicas: 106\n",
      "  Grupos de duplicados: 0\n",
      "  Total duplicados: 0\n",
      "\n",
      "Analizando clase: 4\n",
      "  Total imágenes en 4: 27\n",
      "  Imágenes únicas: 27\n",
      "  Grupos de duplicados: 0\n",
      "  Total duplicados: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import hashlib\n",
    "\n",
    "def find_duplicate_images(base_path):\n",
    "    # Diccionario para almacenar hashes de imágenes por clase\n",
    "    hashes_by_class = {}\n",
    "    # Diccionario para almacenar imágenes duplicadas por clase\n",
    "    duplicates_by_class = {}\n",
    "    \n",
    "    # Recorre cada clase (directorio)\n",
    "    for split in os.listdir(base_path):\n",
    "        split_path = os.path.join(base_path, split)\n",
    "        for class_name in os.listdir(split_path):\n",
    "            class_path = os.path.join(split_path, class_name)\n",
    "            if not os.path.isdir(class_path):\n",
    "                continue\n",
    "            \n",
    "            # Inicializa el diccionario de hashes para esta clase\n",
    "            hashes = {}\n",
    "            duplicates = defaultdict(list)\n",
    "            \n",
    "            print(f\"Analizando clase: {class_name}\")\n",
    "            \n",
    "            # Recorre cada imagen en la clase\n",
    "            for img_name in os.listdir(class_path):\n",
    "                img_path = os.path.join(class_path, img_name)\n",
    "                if not os.path.isfile(img_path):\n",
    "                    continue\n",
    "                \n",
    "                # Lee la imagen\n",
    "                try:\n",
    "                    img = cv2.imread(img_path)\n",
    "                    if img is None:\n",
    "                        print(f\"  Error al leer: {img_path}\")\n",
    "                        continue\n",
    "                    \n",
    "                    # Calcular hash de la imagen (usando el array de bytes directamente)\n",
    "                    img_hash = hashlib.md5(img.tobytes()).hexdigest()\n",
    "                    \n",
    "                    # Comprobar si este hash ya existe\n",
    "                    if img_hash in hashes:\n",
    "                        # Añadir a la lista de duplicados\n",
    "                        duplicates[img_hash].append(img_name)\n",
    "                        duplicates[img_hash].append(hashes[img_hash]) if hashes[img_hash] not in duplicates[img_hash] else None\n",
    "                        print(f\"  Imagen duplicada encontrada: {img_name} es igual a {hashes[img_hash]}\")\n",
    "                    else:\n",
    "                        # Guardar el hash con el nombre del archivo\n",
    "                        hashes[img_hash] = img_name\n",
    "                except Exception as e:\n",
    "                    print(f\"  Error procesando {img_path}: {e}\")\n",
    "            \n",
    "            # Guardar resultados para esta clase\n",
    "            hashes_by_class[class_name] = hashes\n",
    "            duplicates_by_class[class_name] = duplicates\n",
    "            \n",
    "            # Mostrar estadísticas para esta clase\n",
    "            print(f\"  Total imágenes en {class_name}: {len(hashes) + sum(len(d) for d in duplicates.values())}\")\n",
    "            print(f\"  Imágenes únicas: {len(hashes)}\")\n",
    "            print(f\"  Grupos de duplicados: {len(duplicates)}\")\n",
    "            print(f\"  Total duplicados: {sum(len(d) for d in duplicates.values())}\")\n",
    "            print()\n",
    "        \n",
    "    return hashes_by_class, duplicates_by_class\n",
    "\n",
    "# Para visualizar algunas imágenes duplicadas (opcional)\n",
    "def visualize_duplicates(base_path, duplicates_by_class, max_samples=3):\n",
    "    for class_name, duplicates in duplicates_by_class.items():\n",
    "        if not duplicates:\n",
    "            continue\n",
    "            \n",
    "        class_path = os.path.join(base_path, class_name)\n",
    "        \n",
    "        # Muestra hasta max_samples grupos de duplicados\n",
    "        samples = list(duplicates.items())[:max_samples]\n",
    "        \n",
    "        for i, (img_hash, duplicate_names) in enumerate(samples):\n",
    "            n_duplicates = len(duplicate_names)\n",
    "            fig, axes = plt.subplots(1, min(n_duplicates, 5), figsize=(15, 4))\n",
    "            \n",
    "            if n_duplicates == 1:\n",
    "                axes = [axes]  # Convertir a lista si solo hay una imagen\n",
    "            \n",
    "            fig.suptitle(f\"Clase {class_name}: Grupo de duplicados {i+1}\", fontsize=16)\n",
    "            \n",
    "            # Mostrar hasta 5 imágenes duplicadas\n",
    "            for j, img_name in enumerate(duplicate_names[:5]):\n",
    "                img_path = os.path.join(class_path, img_name)\n",
    "                img = cv2.imread(img_path)\n",
    "                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                axes[j].imshow(img)\n",
    "                axes[j].set_title(img_name)\n",
    "                axes[j].axis('off')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "# Ejecutar el análisis\n",
    "hashes_by_class, duplicates_by_class = find_duplicate_images(MENDELEY_OAI_224_SPLIT_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╒═════════╤══════╤══════╤══════╤═════╤═════╕\n",
      "│ Clase   │    0 │    1 │    2 │   3 │   4 │\n",
      "╞═════════╪══════╪══════╪══════╪═════╪═════╡\n",
      "│ train   │ 2892 │ 1327 │ 1933 │ 964 │ 221 │\n",
      "├─────────┼──────┼──────┼──────┼─────┼─────┤\n",
      "│ val     │  308 │  141 │  206 │ 102 │  23 │\n",
      "├─────────┼──────┼──────┼──────┼─────┼─────┤\n",
      "│ test    │  657 │  302 │  439 │ 220 │  51 │\n",
      "╘═════════╧══════╧══════╧══════╧═════╧═════╛\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'train': [2892, 1327, 1933, 964, 221],\n",
       " 'val': [308, 141, 206, 102, 23],\n",
       " 'test': [657, 302, 439, 220, 51]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = explorar_split_data('dataset/experimento1/split_oai')\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = explorar_split_data(AUG)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
