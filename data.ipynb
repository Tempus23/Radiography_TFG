{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importar librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from src.config import *\n",
    "from src.data import *\n",
    "from src.utils import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mendeley 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explorar_split_data(path):\n",
    "    imagenes = {}\n",
    "    for subset in [\"train\", \"val\", \"test\", \"auto_test\"]:\n",
    "        subset_path = os.path.join(path, subset)\n",
    "        if not os.path.exists(subset_path):\n",
    "            continue\n",
    "        if os.path.exists(subset_path):\n",
    "            imagenes[subset] = []  # Inicializamos la lista para cada subset\n",
    "            for root, dirs, files in os.walk(subset_path):\n",
    "                # Omitimos el directorio raíz\n",
    "                if root == subset_path:\n",
    "                    continue\n",
    "                imagenes[subset].append(len(files))\n",
    "    print_split_table(imagenes)\n",
    "    return imagenes\n",
    "def print_split_table(data):\n",
    "    num_clases = len(data['train'])\n",
    "    tabla = []\n",
    "\n",
    "    for i in [\"train\", \"val\", \"test\", \"auto_test\"]:\n",
    "        fila = [f\"{i}\", data[i][0], data[i][1], data[i][2], data[i][3], data[i][4]]\n",
    "        tabla.append(fila)\n",
    "\n",
    "    print(tabulate(tabla, headers=[\"Clase\", \"0\", \"1\", \"2\", \"3\", \"4\"], tablefmt=\"fancy_grid\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = explorar_split_data(MENDELEY_OAI_224_SPLIT_PATH)\n",
    "TOTAL = sum([sum(img[split]) for split in img])\n",
    "num_por_clase = [0,0,0,0,0]\n",
    "for split in img:\n",
    "    print(f\"Split {split}: {sum(img[split])} - {sum(img[split])/TOTAL*100:.2f}%\") \n",
    "    num_por_clase = [num_por_clase[i] + img[split][i] for i in range(5)]\n",
    "\n",
    "\n",
    "print(TOTAL)\n",
    "print(num_por_clase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import hashlib\n",
    "\n",
    "def find_duplicate_images(base_path):\n",
    "    # Diccionario para almacenar hashes de imágenes por clase\n",
    "    hashes_by_class = {}\n",
    "    # Diccionario para almacenar imágenes duplicadas por clase\n",
    "    duplicates_by_class = {}\n",
    "    \n",
    "    # Recorre cada clase (directorio)\n",
    "    for split in os.listdir(base_path):\n",
    "        split_path = os.path.join(base_path, split)\n",
    "        for class_name in os.listdir(split_path):\n",
    "            class_path = os.path.join(split_path, class_name)\n",
    "            if not os.path.isdir(class_path):\n",
    "                continue\n",
    "            \n",
    "            # Inicializa el diccionario de hashes para esta clase\n",
    "            hashes = {}\n",
    "            duplicates = defaultdict(list)\n",
    "            \n",
    "            print(f\"Analizando clase: {class_name}\")\n",
    "            \n",
    "            # Recorre cada imagen en la clase\n",
    "            for img_name in os.listdir(class_path):\n",
    "                img_path = os.path.join(class_path, img_name)\n",
    "                if not os.path.isfile(img_path):\n",
    "                    continue\n",
    "                \n",
    "                # Lee la imagen\n",
    "                try:\n",
    "                    img = cv2.imread(img_path)\n",
    "                    if img is None:\n",
    "                        print(f\"  Error al leer: {img_path}\")\n",
    "                        continue\n",
    "                    \n",
    "                    # Calcular hash de la imagen (usando el array de bytes directamente)\n",
    "                    img_hash = hashlib.md5(img.tobytes()).hexdigest()\n",
    "                    \n",
    "                    # Comprobar si este hash ya existe\n",
    "                    if img_hash in hashes:\n",
    "                        # Añadir a la lista de duplicados\n",
    "                        duplicates[img_hash].append(img_name)\n",
    "                        duplicates[img_hash].append(hashes[img_hash]) if hashes[img_hash] not in duplicates[img_hash] else None\n",
    "                        print(f\"  Imagen duplicada encontrada: {img_name} es igual a {hashes[img_hash]}\")\n",
    "                    else:\n",
    "                        # Guardar el hash con el nombre del archivo\n",
    "                        hashes[img_hash] = img_name\n",
    "                except Exception as e:\n",
    "                    print(f\"  Error procesando {img_path}: {e}\")\n",
    "            \n",
    "            # Guardar resultados para esta clase\n",
    "            hashes_by_class[class_name] = hashes\n",
    "            duplicates_by_class[class_name] = duplicates\n",
    "            \n",
    "            # Mostrar estadísticas para esta clase\n",
    "            print(f\"  Total imágenes en {class_name}: {len(hashes) + sum(len(d) for d in duplicates.values())}\")\n",
    "            print(f\"  Imágenes únicas: {len(hashes)}\")\n",
    "            print(f\"  Grupos de duplicados: {len(duplicates)}\")\n",
    "            print(f\"  Total duplicados: {sum(len(d) for d in duplicates.values())}\")\n",
    "            print()\n",
    "        \n",
    "    return hashes_by_class, duplicates_by_class\n",
    "\n",
    "# Para visualizar algunas imágenes duplicadas (opcional)\n",
    "def visualize_duplicates(base_path, duplicates_by_class, max_samples=3):\n",
    "    for class_name, duplicates in duplicates_by_class.items():\n",
    "        if not duplicates:\n",
    "            continue\n",
    "            \n",
    "        class_path = os.path.join(base_path, class_name)\n",
    "        \n",
    "        # Muestra hasta max_samples grupos de duplicados\n",
    "        samples = list(duplicates.items())[:max_samples]\n",
    "        \n",
    "        for i, (img_hash, duplicate_names) in enumerate(samples):\n",
    "            n_duplicates = len(duplicate_names)\n",
    "            fig, axes = plt.subplots(1, min(n_duplicates, 5), figsize=(15, 4))\n",
    "            \n",
    "            if n_duplicates == 1:\n",
    "                axes = [axes]  # Convertir a lista si solo hay una imagen\n",
    "            \n",
    "            fig.suptitle(f\"Clase {class_name}: Grupo de duplicados {i+1}\", fontsize=16)\n",
    "            \n",
    "            # Mostrar hasta 5 imágenes duplicadas\n",
    "            for j, img_name in enumerate(duplicate_names[:5]):\n",
    "                img_path = os.path.join(class_path, img_name)\n",
    "                img = cv2.imread(img_path)\n",
    "                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                axes[j].imshow(img)\n",
    "                axes[j].set_title(img_name)\n",
    "                axes[j].axis('off')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "# Ejecutar el análisis\n",
    "hashes_by_class, duplicates_by_class = find_duplicate_images(MENDELEY_OAI_224_SPLIT_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h, d = find_duplicate_images(MENDELEY_EXPERT1_SPLIT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = explorar_data(MENDELEY_EXPERT1_PATH)\n",
    "total = 0\n",
    "\n",
    "print(sum(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = explorar_data(MENDELEY_EXPERT2_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = explorar_split_data(MENDELEY_EXPERT1_SPLIT_PATH)\n",
    "mostrar_imagenes(os.path.join(MENDELEY_EXPERT1_SPLIT_PATH, 'train'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = explorar_split_data(MENDELEY_OAI_BRIGHT_200_SPLIT_PATH)\n",
    "mostrar_imagenes(MENDELEY_OAI_BRIGHT_200_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = explorar_split_data(AUG)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
