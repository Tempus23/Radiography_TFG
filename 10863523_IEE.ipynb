{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper\n",
    "## Imports\n",
    "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10863523\n",
    "\n",
    "bib reference -> 10863523"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator # type: ignore\n",
    "\n",
    "from src.config import *\n",
    "from src.data import *\n",
    "from src.models.efficientnet import EfficientNetB5Custom\n",
    "from src.utils import *\n",
    "from src.data import OriginalOAIDataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEW_OAI_DATASET = 'dataset/mendeleyOAI_dataset/augmented_dataset_1'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original dataset\n",
    "data = explorar_split_data(MENDELEY_OAI_224_SPLIT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ORIGINAL_TRAIN_PATH = os.path.join(MENDELEY_OAI_224_SPLIT_PATH, 'train')\n",
    "classes = [d for d in os.listdir(ORIGINAL_TRAIN_PATH) if os.path.isdir(os.path.join(ORIGINAL_TRAIN_PATH, d))]\n",
    "data_gen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "if not os.path.exists(NEW_OAI_DATASET):\n",
    "    os.makedirs(NEW_OAI_DATASET)\n",
    "\n",
    "augmentation_classes = [1, 2, 5, 10, 20]\n",
    "for class_name in classes:\n",
    "    class_dir = os.path.join(ORIGINAL_TRAIN_PATH, class_name)\n",
    "    print(int(class_name))\n",
    "    num_augmentations = augmentation_classes[int(class_name)]\n",
    "    print(f\"Generando imágenes aumentadas para la clase {class_name}...\")\n",
    "    print(f\"Se generarán {num_augmentations} imágenes\")\n",
    "    print(f\"Directorio original de la clase: {class_dir}\")\n",
    "    imagenes_generadas = 0\n",
    "    for img_name in os.listdir(class_dir):\n",
    "        \n",
    "        \n",
    "        img_path = os.path.join(class_dir, img_name)\n",
    "        img = cv2.imread(img_path)\n",
    "        \n",
    "        # Verificar si la imagen fue leída correctamente\n",
    "        if img is None:\n",
    "            print(f\"Error al leer la imagen {img_path}. Puede que no sea una imagen válida o esté dañada.\")\n",
    "            continue\n",
    "        \n",
    "        # Convertir la imagen a un numpy array\n",
    "        img_array = np.array(img)\n",
    "        img_array = img_array.reshape((1,) + img_array.shape)  # Añadir dimensión batch\n",
    "        # Generar imágenes aumentadas\n",
    "        \"\"\"\n",
    "        for i in range(num_augmentations):\n",
    "            for batch in data_gen.flow(img_array, batch_size=1, save_to_dir=NEW_OAI_DATASET, save_prefix='aug', save_format='png'):\n",
    "                break\n",
    "        \"\"\"\n",
    "        imagenes_generadas += num_augmentations\n",
    "\n",
    "    print(f\"Se han generado {imagenes_generadas} imágenes aumentadas para la clase {class_name}\\n-----------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local mode, 100 images loaded\n",
      "Local mode, 100 images loaded\n",
      "Local mode, 100 images loaded\n",
      "Local mode, 100 images loaded\n",
      "Local mode, 100 images loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\34658\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\34658\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B5_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B5_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_dataloader = OriginalOAIDataset(mode='train', transform=transform,local=True).get_dataloader(batch_size=BATCH_SIZE)\n",
    "\n",
    "model = EfficientNetB5Custom(num_classes = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "4\n",
      "3\n",
      "3\n",
      "1\n",
      "0\n",
      "2\n",
      "4\n",
      "0\n",
      "2\n",
      "4\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "3\n",
      "3\n",
      "3\n",
      "1\n",
      "1\n",
      "3\n",
      "2\n",
      "2\n",
      "0\n",
      "1\n",
      "4\n",
      "0\n",
      "2\n",
      "4\n",
      "1\n",
      "3\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_dataloader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1, 224, 224])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(data[0][0])\n",
    "transform(image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
