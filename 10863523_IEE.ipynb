{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper\n",
    "## Imports\n",
    "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10863523\n",
    "\n",
    "bib reference -> 10863523"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator # type: ignore\n",
    "\n",
    "from src.config import *\n",
    "from src.data import *\n",
    "from src.models.efficientnet import EfficientNetB5Custom\n",
    "from src.utils import *\n",
    "from src.data import OriginalOAIDataset\n",
    "from src.train import train, train_model\n",
    "from src.trainers.classification import Classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEW_OAI_DATASET = 'dataset/mendeleyOAI_dataset/augmented_dataset_1'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╒═════════╤══════╤══════╤══════╤═════╤═════╕\n",
      "│ Clase   │    0 │    1 │    2 │   3 │   4 │\n",
      "╞═════════╪══════╪══════╪══════╪═════╪═════╡\n",
      "│ train   │ 2286 │ 1046 │ 1516 │ 757 │ 173 │\n",
      "├─────────┼──────┼──────┼──────┼─────┼─────┤\n",
      "│ val     │  328 │  153 │  212 │ 106 │  27 │\n",
      "├─────────┼──────┼──────┼──────┼─────┼─────┤\n",
      "│ test    │  639 │  296 │  447 │ 223 │  51 │\n",
      "╘═════════╧══════╧══════╧══════╧═════╧═════╛\n",
      "╒═════════╤══════╤══════╤══════╤══════╤══════╕\n",
      "│ Clase   │    0 │    1 │    2 │    3 │    4 │\n",
      "╞═════════╪══════╪══════╪══════╪══════╪══════╡\n",
      "│ train   │ 1000 │ 1000 │ 1000 │ 1000 │ 1000 │\n",
      "├─────────┼──────┼──────┼──────┼──────┼──────┤\n",
      "│ val     │  328 │  153 │  212 │  106 │   27 │\n",
      "├─────────┼──────┼──────┼──────┼──────┼──────┤\n",
      "│ test    │  639 │  296 │  447 │  223 │   51 │\n",
      "╘═════════╧══════╧══════╧══════╧══════╧══════╛\n"
     ]
    }
   ],
   "source": [
    "# Original dataset\n",
    "data = explorar_split_data(MENDELEY_OAI_224_SPLIT_PATH)\n",
    "train_data = explorar_split_data(NEW_OAI_DATASET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bajar las imagenes a 1000 en las clases 0, 1 y 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bajar a 1000 imagenes las clases 0, 1 y 2\n",
    "ORIGINAL_TRAIN_PATH = os.path.join(MENDELEY_OAI_224_SPLIT_PATH, 'train')\n",
    "classes = [\"0\", \"1\", \"2\"]\n",
    "\n",
    "TRAIN_PATH = os.path.join(NEW_OAI_DATASET, 'train')\n",
    "if not os.path.exists(TRAIN_PATH):\n",
    "    os.makedirs(TRAIN_PATH)\n",
    "\n",
    "for class_name in classes:\n",
    "    imagenes_totales = 0\n",
    "    class_dir = os.path.join(ORIGINAL_TRAIN_PATH, class_name)\n",
    "    CLASS_PATH = os.path.join(TRAIN_PATH, class_name)\n",
    "    if not os.path.exists(CLASS_PATH):\n",
    "        os.makedirs(CLASS_PATH)\n",
    "    # Probabilidad de mantener la imagen\n",
    "    prob = (1000 / len(os.listdir(class_dir)) ) + 0.05\n",
    "    for img_name in os.listdir(class_dir):\n",
    "        \n",
    "        \n",
    "        img_path = os.path.join(class_dir, img_name)\n",
    "        img = cv2.imread(img_path)\n",
    "        \n",
    "        # Verificar si la imagen fue leída correctamente\n",
    "        if img is None:\n",
    "            print(f\"Error al leer la imagen {img_path}. Puede que no sea una imagen válida o esté dañada.\")\n",
    "            continue\n",
    "        \n",
    "        # Aplicar la probabilidad\n",
    "        if np.random.rand() > prob or imagenes_totales >= 1000:\n",
    "            continue\n",
    "        \n",
    "        # Copiar la imagen\n",
    "        new_img_path = os.path.join(CLASS_PATH, f\"{class_name}_{img_name}\")\n",
    "        cv2.imwrite(new_img_path, img)\n",
    "                    \n",
    "        imagenes_totales += 1\n",
    "\n",
    "    print(f\"Clase {class_name} con {imagenes_totales} imágenes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generar imagenes hasta llegar a 1000 en las clases 3 y 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ORIGINAL_TRAIN_PATH = os.path.join(MENDELEY_OAI_224_SPLIT_PATH, 'train')\n",
    "classes = [str(i) for i in range(3,5)]\n",
    "print(classes)\n",
    "data_gen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "if not os.path.exists(NEW_OAI_DATASET):\n",
    "    os.makedirs(NEW_OAI_DATASET)\n",
    "TRAIN_PATH = os.path.join(NEW_OAI_DATASET, 'train')\n",
    "if not os.path.exists(TRAIN_PATH):\n",
    "    os.makedirs(TRAIN_PATH)\n",
    "augmentation_classes = [1, 2, 5, 10, 20]\n",
    "for class_name in classes:\n",
    "    class_dir = os.path.join(ORIGINAL_TRAIN_PATH, class_name)\n",
    "    CLASS_PATH = os.path.join(TRAIN_PATH, class_name)\n",
    "    if not os.path.exists(CLASS_PATH):\n",
    "        os.makedirs(CLASS_PATH)\n",
    "\n",
    "    num_augmentations = 1000 - len(os.listdir(class_dir))\n",
    "    print(f\"Copiando imagenes de la clase {class_name}...\")\n",
    "    for img_name in os.listdir(class_dir):\n",
    "        \n",
    "        \n",
    "        img_path = os.path.join(class_dir, img_name)\n",
    "        img = cv2.imread(img_path)\n",
    "        \n",
    "        # Verificar si la imagen fue leída correctamente\n",
    "        if img is None:\n",
    "            print(f\"Error al leer la imagen {img_path}. Puede que no sea una imagen válida o esté dañada.\")\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        # Copiar la imagen\n",
    "        new_img_path = os.path.join(CLASS_PATH, f\"{class_name}_{img_name}\")\n",
    "        cv2.imwrite(new_img_path, img)\n",
    "\n",
    "\n",
    "    print(f\"Se han copiado {len(os.listdir(CLASS_PATH))} imágenes de la clase {class_name}\")\n",
    "    print(f\"Generando imágenes aumentadas para la clase {class_name}...\")\n",
    "    \n",
    "    while(len(os.listdir(CLASS_PATH)) < 1000):\n",
    "        \n",
    "        probabilidad = ((1000 - len(os.listdir(CLASS_PATH))) / len(os.listdir(CLASS_PATH))) + 0.05\n",
    "        print(f\"Probabilidad: {probabilidad}\")\n",
    "        for img_name in os.listdir(class_dir):\n",
    "            if len(os.listdir(CLASS_PATH)) >= 1000:\n",
    "                break\n",
    "            if np.random.rand() > probabilidad:\n",
    "                continue\n",
    "            \n",
    "            img_path = os.path.join(class_dir, img_name)\n",
    "            img = cv2.imread(img_path)\n",
    "            img_array = img.reshape((1, ) + img.shape)\n",
    "            for batch in data_gen.flow(img_array, batch_size=1, save_to_dir=CLASS_PATH, save_prefix='aug', save_format='png'):\n",
    "                break\n",
    "            \n",
    "            \n",
    "\n",
    "    print(f\"Se han generado {len(os.listdir(CLASS_PATH))} imágenes aumentadas para la clase {class_name}\\n-----------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copiar imagenes de val y test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bajar a 1000 imagenes las clases 0, 1 y 2\n",
    "ORIGINAL_VAL_PATH = os.path.join(MENDELEY_OAI_224_SPLIT_PATH, 'val')\n",
    "ORIGINAL_TEST_PATH = os.path.join(MENDELEY_OAI_224_SPLIT_PATH, 'test')\n",
    "classes = [str(i) for i in range(5)]\n",
    "\n",
    "NEW_VAL_PATH = os.path.join(NEW_OAI_DATASET, 'val')\n",
    "NEW_TEST_PATH = os.path.join(NEW_OAI_DATASET, 'test')\n",
    "\n",
    "# Copiar val\n",
    "for class_name in classes:\n",
    "    class_dir = os.path.join(ORIGINAL_VAL_PATH, class_name)\n",
    "    CLASS_PATH = os.path.join(NEW_VAL_PATH, class_name)\n",
    "    if not os.path.exists(CLASS_PATH):\n",
    "        os.makedirs(CLASS_PATH)\n",
    "    for img_name in os.listdir(class_dir):\n",
    "        img_path = os.path.join(class_dir, img_name)\n",
    "        img = cv2.imread(img_path)\n",
    "        new_img_path = os.path.join(CLASS_PATH, f\"{class_name}_{img_name}\")\n",
    "        cv2.imwrite(new_img_path, img)\n",
    "\n",
    "# Copiar test\n",
    "for class_name in classes:\n",
    "    class_dir = os.path.join(ORIGINAL_TEST_PATH, class_name)\n",
    "    CLASS_PATH = os.path.join(NEW_TEST_PATH, class_name)\n",
    "    if not os.path.exists(CLASS_PATH):\n",
    "        os.makedirs(CLASS_PATH)\n",
    "    for img_name in os.listdir(class_dir):\n",
    "        img_path = os.path.join(class_dir, img_name)\n",
    "        img = cv2.imread(img_path)\n",
    "        new_img_path = os.path.join(CLASS_PATH, f\"{class_name}_{img_name}\")\n",
    "        cv2.imwrite(new_img_path, img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "BATCH_SIZE = 5\n",
    "LEARNING_RATE = 0.01\n",
    "FACTOR = 0.001\n",
    "L1 = 0.00\n",
    "L2 = 0.00\n",
    "PATIENCE = 5\n",
    "BETAS=(0.9, 0.999)\n",
    "# Regularización L1 y L2\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_dataset = OriginalOAIDataset('train', batch_size=BATCH_SIZE, transform=transform, local=True)\n",
    "val_dataset = OriginalOAIDataset('val', batch_size=BATCH_SIZE, transform=transform, local=True)\n",
    "model = EfficientNetB5Custom(num_classes = 5)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "trainer = Classification(model, device, L1=L1, L2=L2, lr=LEARNING_RATE, factor=FACTOR, patience=PATIENCE, betas=BETAS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_model(model, trainer, train_dataset, val_dataset, epochs=2, device=device)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
